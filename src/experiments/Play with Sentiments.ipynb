{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =   '/DATA/arabic_training/pg_new_train.xlsx'\n",
    "data = pd.read_excel(file_path, encoding='utf-8').astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of desired executors: 1\n",
      "Number of desired processes / executor: 3\n",
      "Total number of workers: 3\n"
     ]
    }
   ],
   "source": [
    "# Modify these variables according to your needs.\n",
    "application_name = \"Play with Sentiments\"\n",
    "\n",
    "master = \"local[*]\"\n",
    "num_processes = 3\n",
    "num_executors = 1\n",
    "\n",
    "# This variable is derived from the number of cores and executors,\n",
    "# and will be used to assign the number of model trainers.\n",
    "num_workers = num_executors * num_processes\n",
    "\n",
    "print(\"Number of desired executors: \" + str(num_executors))\n",
    "print(\"Number of desired processes / executor: \" + str(num_processes))\n",
    "print(\"Total number of workers: \" + str(num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", num_processes)\n",
    "conf.set(\"spark.executor.instances\", num_executors)\n",
    "conf.set(\"spark.executor.memory\", \"5g\")\n",
    "conf.set(\"spark.locality.wait\", \"0\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "conf.set(\"spark.kryoserializer.buffer.max\", \"2000\")\n",
    "conf.set(\"spark.executor.heartbeatInterval\", \"6000s\")\n",
    "conf.set(\"spark.network.timeout\", \"10000000s\")\n",
    "conf.set(\"spark.shuffle.spill\", \"true\")\n",
    "conf.set(\"spark.driver.memory\", \"10g\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"10g\")\n",
    "\n",
    "reader = SparkSession.builder.config(conf=conf) \\\n",
    "                     .appName(application_name) \\\n",
    "                     .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             Message|Sentiment|\n",
      "+--------------------+---------+\n",
      "|-----------------...|  Neutral|\n",
      "|....tend to use M...| Negative|\n",
      "|\"1 Billion CFU fr...|  Neutral|\n",
      "|\"Combining vitami...| Positive|\n",
      "|\"Fasting glucose ...|  Neutral|\n",
      "|\"I brought Fiber ...| Negative|\n",
      "|\"no shoes, no bra...| Positive|\n",
      "|\"One time I was h...|  Neutral|\n",
      "|\"Prayer will neve...| Negative|\n",
      "|\"Researchers may ...|  Neutral|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "148544\n"
     ]
    }
   ],
   "source": [
    "raw = reader.createDataFrame(data)\n",
    "raw.show(10)\n",
    "print(raw.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def regex_or(*items):\n",
    "    return '(?:' + '|'.join(items) + ')'\n",
    "\n",
    "contractions = re.compile(\"(?i)(\\w+)(n[''′]t|[''′]ve|[''′]ll|[''′]d|[''′]re|[''′]s|[''′]m)$\", re.UNICODE)\n",
    "whitespace = re.compile(\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
    "punct_chars = r\"['\\\"\"\"''.?!…,:;]\"\n",
    "# punct_seq = punct_chars + \"+\"    # 'anthem'. => ' anthem '.\n",
    "punct_seq = r\"['\\\"\"\"'']+|[.?!,…]+|[:;]+\"  # 'anthem'. => ' anthem ' .\n",
    "entity = r\"[&<>\\\"]\"\n",
    "url_start_1 = r\"(?:https?://|\\bwww\\.)\"\n",
    "commonTLDs = r\"(?:com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|pro|tel|travel|xxx)\"\n",
    "cc_tlds = \\\n",
    "    r\"(?:ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|\" \\\n",
    "    r\"bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|\" \\\n",
    "    r\"er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|\" \\\n",
    "    r\"hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|\" \\\n",
    "    r\"lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|\" \\\n",
    "    r\"nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|\" \\\n",
    "    r\"sl|sm|sn|so|sr|ss|st|su|sv|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|\" \\\n",
    "    r\"va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)\"\n",
    "url_start_2 = \\\n",
    "    r\"\\b(?:[A-Za-z\\d-])+(?:\\.[A-Za-z0-9]+){0,3}\\.\" + regex_or(commonTLDs, cc_tlds) + r\"(?:\\.\" + cc_tlds + r\")?(?=\\W|$)\"\n",
    "url_body = r\"(?:[^\\.\\s<>][^\\s<>]*?)?\"\n",
    "url_extra_crap_before_end = regex_or(punct_chars, entity) + \"+?\"\n",
    "url_end = r\"(?:\\.\\.+|[<>]|\\s|$)\"\n",
    "URL_REGEX = regex_or(url_start_1, url_start_2) + url_body + \"(?=(?:\" + url_extra_crap_before_end + \")?\" + url_end + \")\"\n",
    "USER_ID_REGEX = \"@\\S+\"\n",
    "PUNCTUATION_REGEX=\"[@\\:;`~=\\+&^%,\\'\\\"#\\{\\}\\(\\)\\[\\]\\-\\*/|]\"\n",
    "HTML_REGEX= \"<.*?>\"\n",
    "SPACE_REGEX=\" +\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_with_space = regex_or(URL_REGEX,USER_ID_REGEX,PUNCTUATION_REGEX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             Message|Sentiment|\n",
      "+--------------------+---------+\n",
      "|                 ...|  Neutral|\n",
      "|....tend to use M...| Negative|\n",
      "| 1 Billion CFU fr...|  Neutral|\n",
      "| Combining vitami...| Positive|\n",
      "| Fasting glucose ...|  Neutral|\n",
      "| I brought Fiber ...| Negative|\n",
      "| no shoes  no bra...| Positive|\n",
      "| One time I was h...|  Neutral|\n",
      "| Prayer will neve...| Negative|\n",
      "| Researchers may ...|  Neutral|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|             Message|Sentiment|\n",
      "+--------------------+---------+\n",
      "| Amino acid and p...|  neutral|\n",
      "|....tend to use M...| negative|\n",
      "| 1 Billion CFU fr...|  neutral|\n",
      "| Combining vitami...| positive|\n",
      "| Fasting glucose ...|  neutral|\n",
      "| I brought Fiber ...| negative|\n",
      "| no shoes no bra ...| positive|\n",
      "| One time I was h...|  neutral|\n",
      "| Prayer will neve...| negative|\n",
      "| Researchers may ...|  neutral|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace,col,lower\n",
    "\n",
    "raw = raw.withColumn(\"Message\", regexp_replace(col(\"Message\"), replace_with_space, \" \"))\n",
    "raw.show(10)\n",
    "raw = raw.withColumn(\"Message\", regexp_replace(col(\"Message\"), SPACE_REGEX, \" \"))\n",
    "\n",
    "raw = raw.withColumn(\"Sentiment\", lower(col(\"Sentiment\")));\n",
    "raw.show(10)\n",
    "\n",
    "processed = raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------------+\n",
      "|             Message|Sentiment|        Label|\n",
      "+--------------------+---------+-------------+\n",
      "| Amino acid and p...|  neutral|(2,[1],[1.0])|\n",
      "|....tend to use M...| negative|    (2,[],[])|\n",
      "| 1 Billion CFU fr...|  neutral|(2,[1],[1.0])|\n",
      "| Combining vitami...| positive|(2,[0],[1.0])|\n",
      "| Fasting glucose ...|  neutral|(2,[1],[1.0])|\n",
      "| I brought Fiber ...| negative|    (2,[],[])|\n",
      "| no shoes no bra ...| positive|(2,[0],[1.0])|\n",
      "| One time I was h...|  neutral|(2,[1],[1.0])|\n",
      "| Prayer will neve...| negative|    (2,[],[])|\n",
      "| Researchers may ...|  neutral|(2,[1],[1.0])|\n",
      "| The other thing ...|  neutral|(2,[1],[1.0])|\n",
      "| The other thing ...|  neutral|(2,[1],[1.0])|\n",
      "| There are other ...|  neutral|(2,[1],[1.0])|\n",
      "| Vitamin C antibi...|  neutral|(2,[1],[1.0])|\n",
      "| Vitamin C and an...| positive|(2,[0],[1.0])|\n",
      "| Wayne Boatwright...| negative|    (2,[],[])|\n",
      "|“I’m not lazy and...| negative|    (2,[],[])|\n",
      "|» Cucumber Garlic...|  neutral|(2,[1],[1.0])|\n",
      "| Medical News Tod...| positive|(2,[0],[1.0])|\n",
      "| Mixing Vitamin C...| positive|(2,[0],[1.0])|\n",
      "+--------------------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+---------+-------------+--------------------+\n",
      "|             Message|Sentiment|        Label|              result|\n",
      "+--------------------+---------+-------------+--------------------+\n",
      "| Amino acid and p...|  neutral|(2,[1],[1.0])|[0.01710031491347...|\n",
      "|....tend to use M...| negative|    (2,[],[])|[-0.0050201507718...|\n",
      "| 1 Billion CFU fr...|  neutral|(2,[1],[1.0])|[0.04364197564843...|\n",
      "| Combining vitami...| positive|(2,[0],[1.0])|[0.01050049290060...|\n",
      "| Fasting glucose ...|  neutral|(2,[1],[1.0])|[0.04288094056149...|\n",
      "| I brought Fiber ...| negative|    (2,[],[])|[0.06116106334805...|\n",
      "| no shoes no bra ...| positive|(2,[0],[1.0])|[0.13715739338658...|\n",
      "| One time I was h...|  neutral|(2,[1],[1.0])|[0.02380400471803...|\n",
      "| Prayer will neve...| negative|    (2,[],[])|[8.86105295074613...|\n",
      "| Researchers may ...|  neutral|(2,[1],[1.0])|[0.03526712753074...|\n",
      "| The other thing ...|  neutral|(2,[1],[1.0])|[0.03564498145636...|\n",
      "| The other thing ...|  neutral|(2,[1],[1.0])|[0.04799104905263...|\n",
      "| There are other ...|  neutral|(2,[1],[1.0])|[0.01651341188699...|\n",
      "| Vitamin C antibi...|  neutral|(2,[1],[1.0])|[0.05239416868425...|\n",
      "| Vitamin C and an...| positive|(2,[0],[1.0])|[0.02981869101916...|\n",
      "| Wayne Boatwright...| negative|    (2,[],[])|[-0.0121276368082...|\n",
      "|“I’m not lazy and...| negative|    (2,[],[])|[0.01295292638307...|\n",
      "|» Cucumber Garlic...|  neutral|(2,[1],[1.0])|[-1.1784485381905...|\n",
      "| Medical News Tod...| positive|(2,[0],[1.0])|[0.04274489012147...|\n",
      "| Mixing Vitamin C...| positive|(2,[0],[1.0])|[0.03233787563577...|\n",
      "+--------------------+---------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer,IndexToString,OneHotEncoderEstimator,VectorIndexer,Word2Vec,Tokenizer\n",
    "\n",
    "string_indexer = StringIndexer().setInputCol(\"Sentiment\").setOutputCol(\"Index\")\n",
    "indexed = string_indexer.fit(processed).transform(processed)\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"Index\"],\n",
    "                                 outputCols=[\"Label\"])\n",
    "encoded = encoder.fit(indexed).transform(indexed).drop(\"Index\")\n",
    "encoded.show()\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"Message\", outputCol=\"Words\")\n",
    "wordsData = tokenizer.transform(encoded)\n",
    "word2Vec = Word2Vec(vectorSize=300, minCount=0, inputCol=\"Words\", outputCol=\"result\")\n",
    "model = word2Vec.fit(wordsData)\n",
    "\n",
    "result = model.transform(wordsData).drop(\"Words\")\n",
    "result.show()\n",
    "# indexer_string = IndexToString().setInputCol(\"Label\").setOutputCol(\"originalCategory\")\n",
    "# unindexed = indexer_string.transform(indexed_df)\n",
    "\n",
    "# unindexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Reshape, LSTM, Activation, SpatialDropout1D, initializers\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.models import Model\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from keras.engine.topology import merge\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers.core import Dropout, Dense, Lambda, Masking\n",
    "from keras.layers import merge\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import constraints, regularizers\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.optimizers import Adam, Nadam, SGD, RMSprop\n",
    "\n",
    "# class AttentionLayer(Layer):\n",
    "#     '''\n",
    "#     Attention layer.\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self, init='glorot_uniform', **kwargs):\n",
    "#         super(AttentionLayer, self).__init__(**kwargs)\n",
    "#         self.supports_masking = True\n",
    "#         self.init = initializers.get(init)\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         input_dim = input_shape[-1]\n",
    "#         self.Uw = self.init((input_dim,))\n",
    "#         self.trainable_weights = [self.Uw]\n",
    "#         super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "#     def compute_mask(self, input, mask):\n",
    "#         return mask\n",
    "\n",
    "#     def call(self, x, mask=None):\n",
    "#         print(self.Uw.shape)\n",
    "#         print(list(range(K.ndim(self.Uw))))\n",
    "#         multData = K.exp((K.dot(x, self.Uw)))\n",
    "#         if mask is not None:\n",
    "#             multData = mask * multData\n",
    "#         output = multData / (K.sum(multData, axis=1) + K.epsilon())[:, None]\n",
    "#         return K.reshape(output, (output.shape[0], output.shape[1], 1))\n",
    "\n",
    "#     def get_output_shape_for(self, input_shape):\n",
    "#         newShape = list(input_shape)\n",
    "#         newShape[-1] = 1\n",
    "#         return tuple(newShape)\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return get_output_shape_for(self,input_shape)\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    '''\n",
    "    Attention layer. \n",
    "    Usage:\n",
    "        lstm_layer = LSTM(dim, return_sequences=True)\n",
    "        attention = AttentionLayer()(lstm_layer)\n",
    "        sentenceEmb = merge([lstm_layer, attention], mode=lambda x:x[1]*x[0], output_shape=lambda x:x[0])\n",
    "        sentenceEmb = Lambda(lambda x:K.sum(x, axis=1), output_shape=lambda x:(x[0],x[2]))(sentenceEmb)\n",
    "    '''\n",
    "    def __init__(self, init='glorot_uniform', kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None,  **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get(init)\n",
    "        self.kernel_initializer = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight((input_shape[-1], 1),\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.kernel_regularizer,\n",
    "                                 constraint=self.kernel_constraint)\n",
    "        self.built = True\n",
    "    \n",
    "    def compute_mask(self, input, mask):\n",
    "        return mask\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        multData =  K.exp(K.dot(x, self.kernel))\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask)\n",
    "            multData = mask*multData\n",
    "\n",
    "        output = multData/(K.sum(multData, axis=1)+K.epsilon())[:,None]\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        newShape = list(input_shape)\n",
    "        newShape[-1] = 1\n",
    "        return tuple(newShape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHierarchicalAttentionModel(maxSeq,\n",
    "                                     embWeights=None, embeddingSize=None, vocabSize=1000,  # embedding\n",
    "                                     recursiveClass=GRU, wordRnnSize=100, sentenceRnnSize=100,  # rnn\n",
    "                                     # wordDenseSize = 100, sentenceHiddenSize = 128, #dense\n",
    "                                     dropWordEmb=0.5, dropWordRnnOut=0.5, dropSentenceRnnOut=0.5, nb_classes=3):\n",
    "    wordsInputs = Input(shape=(maxSeq,), name='words_input')\n",
    "    if embWeights is None:\n",
    "        emb = Embedding(vocabSize, 300,mask_zero=True,embeddings_initializer='glorot_uniform')(wordsInputs)\n",
    "    else:\n",
    "        emb = Embedding(embWeights.shape[0], embWeights.shape[1],mask_zero=True,  weights=[embWeights])(wordsInputs)\n",
    "    if dropWordEmb != 0.0:\n",
    "        emb = Dropout(dropWordEmb)(emb)\n",
    "    reg=l1_l2(l2=0.001)\n",
    "    wordRnn = Bidirectional(recursiveClass(wordRnnSize, return_sequences=True), merge_mode='concat')(emb)\n",
    "\n",
    "    if dropWordRnnOut > 0.0:\n",
    "        wordRnn = Dropout(dropWordRnnOut, )(wordRnn)\n",
    "    attention = AttentionLayer()(wordRnn)\n",
    "    sentenceEmb = merge([wordRnn, attention], mode=lambda x: x[1] * x[0], output_shape=lambda x: x[0])\n",
    "    sentenceEmb = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda x: (x[0], x[2]))(sentenceEmb)\n",
    "    modelSentence = Model(wordsInputs, sentenceEmb)\n",
    "    modelSentAttention = Model(wordsInputs, attention)\n",
    "\n",
    "    documentInputs = Input(shape=(None, maxSeq), name='document_input')\n",
    "    sentenceMasking = Masking(mask_value=0)(documentInputs)\n",
    "    sentenceEmbbeding = TimeDistributed(modelSentence)(sentenceMasking)\n",
    "    sentenceAttention = TimeDistributed(modelSentAttention)(sentenceMasking)\n",
    "    sentenceRnn = Bidirectional(recursiveClass(wordRnnSize, return_sequences=True), merge_mode='concat')(\n",
    "        sentenceEmbbeding)\n",
    "    if dropSentenceRnnOut > 0.0:\n",
    "        sentenceRnn = Dropout(dropSentenceRnnOut)(sentenceRnn)\n",
    "    attentionSent = AttentionLayer()(sentenceRnn)\n",
    "    documentEmb = merge([sentenceRnn, attentionSent], mode=lambda x: x[1] * x[0], output_shape=lambda x: x[0])\n",
    "    documentEmb = Lambda(lambda x: K.sum(x, axis=1), output_shape=lambda x: (x[0], x[2]), name=\"att2\")(documentEmb)\n",
    "    documentOut = Dense(nb_classes, activation=\"softmax\", name=\"documentOut\")(documentEmb)\n",
    "\n",
    "    \n",
    "    model = Model(input=[documentInputs], output=[documentOut])\n",
    "    adam=SGD(momentum=0.9)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/opt/conda/lib/python3.6/site-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "keras_model =createHierarchicalAttentionModel(300, embWeights=None, nb_classes=3,\n",
    "                                         vocabSize=model.getVectors().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/DATA/experiments/scatter/src')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer merge_4 was passed non-serializable keyword arguments: {'mask': [<tf.Tensor 'embedding_13/NotEqual:0' shape=(?, 300) dtype=bool>, <tf.Tensor 'embedding_13/NotEqual:0' shape=(?, 300) dtype=bool>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n",
      "/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer merge_5 was passed non-serializable keyword arguments: {'mask': [<tf.Tensor 'masking_3/Any_1:0' shape=(?, ?) dtype=bool>, <tf.Tensor 'masking_3/Any_1:0' shape=(?, ?) dtype=bool>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown layer: AttentionLayer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-f3c7368aff97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommunication_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                    features_col=\"result\", label_col=\"Label\")\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/DATA/experiments/scatter/src/distkeras/trainers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataframe, shuffle)\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter_server\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;31m# Allocate the parameter server.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter_server\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_parameter_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0;31m# Start the communication service.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_service\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DATA/experiments/scatter/src/distkeras/trainers.py\u001b[0m in \u001b[0;36mallocate_parameter_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mthis\u001b[0m \u001b[0mimplementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \"\"\"\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0mparameter_server\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeltaParameterServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparameter_server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DATA/experiments/scatter/src/distkeras/parameter_servers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, master_port)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaster_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeltaParameterServer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaster_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DATA/experiments/scatter/src/distkeras/parameter_servers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, port)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocketParameterServer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_port\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DATA/experiments/scatter/src/distkeras/parameter_servers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialize_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DATA/experiments/scatter/src/distkeras/utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_model\u001b[0;34m(dictionary)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0marchitecture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mmodel_from_json\u001b[0;34m(json_string, custom_objects)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \"\"\"\n\u001b[1;32m    378\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 return cls.from_config(config['config'],\n\u001b[1;32m    143\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 144\u001b[0;31m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   2523\u001b[0m         \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m             \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m         \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2527\u001b[0m         \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   2509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[0;32m-> 2511\u001b[0;31m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m   2512\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 return cls.from_config(config['config'],\n\u001b[1;32m    143\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 144\u001b[0;31m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdeserialize_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         layer = deserialize_layer(config.pop('layer'),\n\u001b[0;32m--> 110\u001b[0;31m                                   custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 return cls.from_config(config['config'],\n\u001b[1;32m    143\u001b[0m                                        custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 144\u001b[0;31m                                                            list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   2523\u001b[0m         \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m             \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m         \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2527\u001b[0m         \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   2509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m             layer = deserialize_layer(layer_data,\n\u001b[0;32m-> 2511\u001b[0;31m                                       custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m   2512\u001b[0m             \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 138\u001b[0;31m                                  ': ' + class_name)\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown layer: AttentionLayer"
     ]
    }
   ],
   "source": [
    "from distkeras.trainers import *\n",
    "from distkeras.predictors import *\n",
    "from distkeras.transformers import *\n",
    "from distkeras.evaluators import *\n",
    "from distkeras.utils import *\n",
    "optimizer_mlp = 'adam'\n",
    "loss_mlp = 'categorical_crossentropy'\n",
    "trainer = DOWNPOUR(keras_model=keras_model, worker_optimizer=optimizer_mlp, loss=loss_mlp, num_workers=num_workers,\n",
    "                   batch_size=4, communication_window=5, num_epoch=1,\n",
    "                   features_col=\"result\", label_col=\"Label\")\n",
    "trained_model = trainer.train(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [-0.059452523291111,-0.04019885137677193,0.03383271899074316]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.07344705558248928,0.027889671070235114,-0.007856659591197968]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.012440059613436461,0.012617905344814063,0.03669551573693752]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = reader.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
